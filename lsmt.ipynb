{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "48/48 [==============================] - 4s 33ms/step - loss: 0.0332 - val_loss: 0.0038\n",
      "Epoch 2/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.0018 - val_loss: 8.9420e-04\n",
      "Epoch 3/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 6.5353e-04 - val_loss: 7.0918e-04\n",
      "Epoch 4/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 5.9171e-04 - val_loss: 6.6576e-04\n",
      "Epoch 5/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 5.4865e-04 - val_loss: 6.3307e-04\n",
      "Epoch 6/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 5.1776e-04 - val_loss: 6.0601e-04\n",
      "Epoch 7/100\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 5.0435e-04 - val_loss: 5.8922e-04\n",
      "Epoch 8/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 4.7353e-04 - val_loss: 5.6440e-04\n",
      "Epoch 9/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 4.5777e-04 - val_loss: 5.5419e-04\n",
      "Epoch 10/100\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 4.5038e-04 - val_loss: 5.5395e-04\n",
      "Epoch 11/100\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 4.4217e-04 - val_loss: 5.5631e-04\n",
      "Epoch 12/100\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 4.2944e-04 - val_loss: 5.1596e-04\n",
      "Epoch 13/100\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 4.2151e-04 - val_loss: 5.0886e-04\n",
      "Epoch 14/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 4.0965e-04 - val_loss: 5.0079e-04\n",
      "Epoch 15/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 4.0105e-04 - val_loss: 4.9574e-04\n",
      "Epoch 16/100\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 3.9482e-04 - val_loss: 4.7555e-04\n",
      "Epoch 17/100\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 3.8970e-04 - val_loss: 4.7592e-04\n",
      "Epoch 18/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 3.8360e-04 - val_loss: 4.5841e-04\n",
      "Epoch 19/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 3.7479e-04 - val_loss: 4.4587e-04\n",
      "Epoch 20/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 3.6618e-04 - val_loss: 4.3192e-04\n",
      "Epoch 21/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 3.5456e-04 - val_loss: 4.1799e-04\n",
      "Epoch 22/100\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 3.5093e-04 - val_loss: 4.1949e-04\n",
      "Epoch 23/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 3.4000e-04 - val_loss: 3.9550e-04\n",
      "Epoch 24/100\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 3.3070e-04 - val_loss: 4.0242e-04\n",
      "Epoch 25/100\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 3.3771e-04 - val_loss: 4.0830e-04\n",
      "Epoch 26/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 3.1939e-04 - val_loss: 3.6082e-04\n",
      "Epoch 27/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 3.1203e-04 - val_loss: 3.7602e-04\n",
      "Epoch 28/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 3.0001e-04 - val_loss: 3.3667e-04\n",
      "Epoch 29/100\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 2.8954e-04 - val_loss: 3.3663e-04\n",
      "Epoch 30/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 2.9973e-04 - val_loss: 3.6394e-04\n",
      "Epoch 31/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 3.3566e-04 - val_loss: 3.1388e-04\n",
      "Epoch 32/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 2.7752e-04 - val_loss: 3.0877e-04\n",
      "Epoch 33/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 2.7954e-04 - val_loss: 3.0469e-04\n",
      "Epoch 34/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 2.6505e-04 - val_loss: 2.8558e-04\n",
      "Epoch 35/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 2.7564e-04 - val_loss: 2.7922e-04\n",
      "Epoch 36/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 2.5599e-04 - val_loss: 2.7031e-04\n",
      "Epoch 37/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 2.6515e-04 - val_loss: 2.6942e-04\n",
      "Epoch 38/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 2.5204e-04 - val_loss: 2.8413e-04\n",
      "Epoch 39/100\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 2.5930e-04 - val_loss: 2.6833e-04\n",
      "Epoch 40/100\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 2.5071e-04 - val_loss: 2.4867e-04\n",
      "Epoch 41/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 2.4808e-04 - val_loss: 2.4366e-04\n",
      "Epoch 42/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 2.5200e-04 - val_loss: 2.7140e-04\n",
      "Epoch 43/100\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 2.4626e-04 - val_loss: 2.3624e-04\n",
      "Epoch 44/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 2.3497e-04 - val_loss: 2.3628e-04\n",
      "Epoch 45/100\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 2.3908e-04 - val_loss: 2.2891e-04\n",
      "Epoch 46/100\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 2.5032e-04 - val_loss: 2.2603e-04\n",
      "Epoch 47/100\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 2.3092e-04 - val_loss: 2.2097e-04\n",
      "Epoch 48/100\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 2.2950e-04 - val_loss: 2.1683e-04\n",
      "Epoch 49/100\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 2.3067e-04 - val_loss: 2.2805e-04\n",
      "Epoch 50/100\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 2.2182e-04 - val_loss: 2.1507e-04\n",
      "Epoch 51/100\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 2.1973e-04 - val_loss: 2.1648e-04\n",
      "Epoch 52/100\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 2.2629e-04 - val_loss: 2.2215e-04\n",
      "Epoch 53/100\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 2.1947e-04 - val_loss: 2.0906e-04\n",
      "Epoch 54/100\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 2.3423e-04 - val_loss: 2.1116e-04\n",
      "Epoch 55/100\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 2.2216e-04 - val_loss: 2.1612e-04\n",
      "Epoch 56/100\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 2.1662e-04 - val_loss: 1.9827e-04\n",
      "Epoch 57/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 2.2630e-04 - val_loss: 1.9582e-04\n",
      "Epoch 58/100\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 2.0909e-04 - val_loss: 1.9783e-04\n",
      "Epoch 59/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 2.1193e-04 - val_loss: 1.9781e-04\n",
      "Epoch 60/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 2.0841e-04 - val_loss: 2.0034e-04\n",
      "Epoch 61/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 2.0846e-04 - val_loss: 1.8771e-04\n",
      "Epoch 62/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 2.0857e-04 - val_loss: 2.0190e-04\n",
      "Epoch 63/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 2.0692e-04 - val_loss: 1.8373e-04\n",
      "Epoch 64/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 2.1165e-04 - val_loss: 1.8145e-04\n",
      "Epoch 65/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 2.0103e-04 - val_loss: 1.8486e-04\n",
      "Epoch 66/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 1.9869e-04 - val_loss: 1.9403e-04\n",
      "Epoch 67/100\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 1.9895e-04 - val_loss: 1.7679e-04\n",
      "Epoch 68/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 1.9538e-04 - val_loss: 1.7463e-04\n",
      "Epoch 69/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 1.9389e-04 - val_loss: 1.8136e-04\n",
      "Epoch 70/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 1.9697e-04 - val_loss: 1.7570e-04\n",
      "Epoch 71/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 1.9035e-04 - val_loss: 1.6910e-04\n",
      "Epoch 72/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 1.9098e-04 - val_loss: 1.8717e-04\n",
      "Epoch 73/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 2.0052e-04 - val_loss: 1.6984e-04\n",
      "Epoch 74/100\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 2.0585e-04 - val_loss: 1.6430e-04\n",
      "Epoch 75/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 1.8988e-04 - val_loss: 1.8946e-04\n",
      "Epoch 76/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 1.9261e-04 - val_loss: 1.6987e-04\n",
      "Epoch 77/100\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 1.9474e-04 - val_loss: 1.7310e-04\n",
      "Epoch 78/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 1.9294e-04 - val_loss: 1.6422e-04\n",
      "Epoch 79/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 1.8214e-04 - val_loss: 2.2345e-04\n",
      "Epoch 80/100\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 1.8806e-04 - val_loss: 1.5903e-04\n",
      "Epoch 81/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 1.8044e-04 - val_loss: 1.5267e-04\n",
      "Epoch 82/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 1.9192e-04 - val_loss: 1.7189e-04\n",
      "Epoch 83/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 1.7648e-04 - val_loss: 1.6692e-04\n",
      "Epoch 84/100\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 1.7852e-04 - val_loss: 1.6362e-04\n",
      "Epoch 85/100\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 1.7871e-04 - val_loss: 1.4610e-04\n",
      "Epoch 86/100\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 1.7674e-04 - val_loss: 1.5227e-04\n",
      "Epoch 87/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 1.7150e-04 - val_loss: 1.5249e-04\n",
      "Epoch 88/100\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 1.8264e-04 - val_loss: 1.4875e-04\n",
      "Epoch 89/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 1.6810e-04 - val_loss: 1.4506e-04\n",
      "Epoch 90/100\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 1.6961e-04 - val_loss: 1.4213e-04\n",
      "Epoch 91/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 1.7031e-04 - val_loss: 1.4935e-04\n",
      "Epoch 92/100\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 1.7081e-04 - val_loss: 1.3772e-04\n",
      "Epoch 93/100\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 1.6594e-04 - val_loss: 1.4211e-04\n",
      "Epoch 94/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 1.6806e-04 - val_loss: 1.3810e-04\n",
      "Epoch 95/100\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 1.7776e-04 - val_loss: 1.3417e-04\n",
      "Epoch 96/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 1.6374e-04 - val_loss: 1.3364e-04\n",
      "Epoch 97/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 1.5900e-04 - val_loss: 1.3628e-04\n",
      "Epoch 98/100\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 1.6351e-04 - val_loss: 1.2908e-04\n",
      "Epoch 99/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 1.5902e-04 - val_loss: 1.5496e-04\n",
      "Epoch 100/100\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 1.5757e-04 - val_loss: 1.2606e-04\n",
      "1/1 [==============================] - 0s 384ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Data: 2023-10-31, Valor previsto: 865.51\n",
      "Data: 2023-11-01, Valor previsto: 866.69\n",
      "Data: 2023-11-02, Valor previsto: 867.77\n",
      "Data: 2023-11-03, Valor previsto: 868.78\n",
      "Data: 2023-11-04, Valor previsto: 869.78\n",
      "Data: 2023-11-05, Valor previsto: 870.74\n",
      "Data: 2023-11-06, Valor previsto: 871.68\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "# Carregar os dados\n",
    "df = pd.read_csv('C:/Users/milen/OneDrive/Documentos/TCC/Bases/base_preview.csv')\n",
    "\n",
    "# Certifique-se de que não há NaNs nos dados\n",
    "df['Preco_Real'].fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Supõe-se que haja uma coluna de data em 'df' que represente as datas correspondentes a 'Preco_Real'\n",
    "df['Data'] = pd.to_datetime(df['Data'])\n",
    "\n",
    "# Definindo 'last_date' com a última data do DataFrame\n",
    "last_date = df['Data'].iloc[-1]\n",
    "\n",
    "# Escalar os dados para o intervalo [0, 1]\n",
    "scaler = MinMaxScaler()\n",
    "df_scaled = scaler.fit_transform(df['Preco_Real'].values.reshape(-1, 1))\n",
    "\n",
    "# Função para criar um conjunto de dados\n",
    "def create_dataset(X, time_steps=1):\n",
    "    Xs = []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        v = X[i:(i + time_steps)]\n",
    "        Xs.append(v)\n",
    "    return np.array(Xs)\n",
    "\n",
    "# Configurações da série temporal\n",
    "time_steps = 30  # Use 30 dias para criar a sequência\n",
    "X = create_dataset(df_scaled, time_steps)\n",
    "y = df_scaled[time_steps:]\n",
    "\n",
    "# Dividir os dados em conjuntos de treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Alterar o número de unidades LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=70, return_sequences=True, input_shape=(X_train.shape[1], 1)))  # Mais unidades\n",
    "model.add(LSTM(units=70))  # Mais unidades\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "# Criar um otimizador com uma taxa de aprendizagem específica\n",
    "optimizer = Adam(learning_rate=0.0001)\n",
    "\n",
    "# Compilar o modelo com o otimizador configurado\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "# Adicionando Early Stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "# Treinar o modelo com early stopping e um batch size diferente\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=64, callbacks=[early_stopping], validation_split=0.1)\n",
    "\n",
    "# Prever os próximos 7 dias\n",
    "# Começar com a última sequência disponível\n",
    "current_sequence = X[-1].reshape((1, time_steps, 1))\n",
    "predicted_next_7_days = []\n",
    "\n",
    "for _ in range(7):\n",
    "    # Fazer previsão utilizando a sequência atual\n",
    "    next_predicted = model.predict(current_sequence)\n",
    "    # Adicionar a previsão à lista\n",
    "    predicted_next_7_days.append(next_predicted[0, 0])\n",
    "    # Atualizar a sequência para incluir a nova previsão\n",
    "    next_predicted = next_predicted.reshape((1, 1, 1))\n",
    "    current_sequence = np.append(current_sequence[:, 1:, :], next_predicted, axis=1)\n",
    "\n",
    "# Escalar de volta as previsões para os valores originais\n",
    "predicted_next_7_days = scaler.inverse_transform(np.array(predicted_next_7_days).reshape(-1, 1))\n",
    "\n",
    "# Criar as datas para os próximos 7 dias\n",
    "next_7_days = [last_date + pd.Timedelta(days=x) for x in range(1, 8)]\n",
    "\n",
    "# Associar as previsões com as datas\n",
    "predictions_with_dates = [(date.date(), value[0]) for date, value in zip(next_7_days, predicted_next_7_days)]\n",
    "\n",
    "# Printar as previsões com as datas correspondentes\n",
    "for date, value in predictions_with_dates:\n",
    "    print(f'Data: {date}, Valor previsto: {value:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/milen/OneDrive/Documentos/TCC/Bases/weather_preview.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\milen\\OneDrive\\Documentos\\GitHub\\TCC\\lsmt.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/milen/OneDrive/Documentos/GitHub/TCC/lsmt.ipynb#W2sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcallbacks\u001b[39;00m \u001b[39mimport\u001b[39;00m EarlyStopping\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/milen/OneDrive/Documentos/GitHub/TCC/lsmt.ipynb#W2sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Carregar os dados\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/milen/OneDrive/Documentos/GitHub/TCC/lsmt.ipynb#W2sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39mC:/Users/milen/OneDrive/Documentos/TCC/Bases/weather_preview.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/milen/OneDrive/Documentos/GitHub/TCC/lsmt.ipynb#W2sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Certifique-se de que não há NaNs nos dados\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/milen/OneDrive/Documentos/GitHub/TCC/lsmt.ipynb#W2sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mPreco_Real\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mfillna(method\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mffill\u001b[39m\u001b[39m'\u001b[39m, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\milen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\milen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\milen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\milen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\milen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Users\\milen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1736\u001b[0m     f,\n\u001b[0;32m   1737\u001b[0m     mode,\n\u001b[0;32m   1738\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1739\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1740\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1741\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1742\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1743\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1744\u001b[0m )\n\u001b[0;32m   1745\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\milen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    859\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    860\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    861\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/milen/OneDrive/Documentos/TCC/Bases/weather_preview.csv'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Carregar os dados\n",
    "df = pd.read_csv('C:/Users/milen/OneDrive/Documentos/TCC/Bases/weather_preview.csv')\n",
    "\n",
    "# Certifique-se de que não há NaNs nos dados\n",
    "df['Preco_Real'].fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Supõe-se que haja uma coluna de data em 'df' que represente as datas correspondentes a 'Preco_Real'\n",
    "df['Data'] = pd.to_datetime(df['Data'])\n",
    "\n",
    "# Definindo 'last_date' com a última data do DataFrame\n",
    "last_date = df['Data'].iloc[-1]\n",
    "\n",
    "# Escalar os dados para o intervalo [0, 1]\n",
    "scaler = MinMaxScaler()\n",
    "df_scaled = scaler.fit_transform(df['Preco_Real'].values.reshape(-1, 1))\n",
    "\n",
    "# Função para criar um conjunto de dados\n",
    "def create_dataset(X, time_steps=1):\n",
    "    Xs = []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        v = X[i:(i + time_steps)]\n",
    "        Xs.append(v)\n",
    "    return np.array(Xs)\n",
    "\n",
    "# Configurações da série temporal\n",
    "time_steps = 10  # Use 30 dias para criar a sequência\n",
    "X = create_dataset(df_scaled, time_steps)\n",
    "y = df_scaled[time_steps:]\n",
    "\n",
    "# Dividir os dados em conjuntos de treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Criar o modelo LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=60, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "model.add(LSTM(units=60))\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "# Criar um otimizador com uma taxa de aprendizagem específica\n",
    "optimizer = Adam(learning_rate=0.0001)\n",
    "\n",
    "# Compilar o modelo com o otimizador configurado\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "# Adicionando Early Stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "# Treinar o modelo com early stopping e um batch size diferente\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=50, callbacks=[early_stopping], validation_split=0.1)\n",
    "\n",
    "# Prever os próximos dias úteis\n",
    "current_sequence = X[-1].reshape((1, time_steps, 1))\n",
    "predicted_next_days = []\n",
    "\n",
    "current_date = last_date\n",
    "\n",
    "for _ in range(7):\n",
    "    for _ in range(7):\n",
    "        current_date += pd.Timedelta(days=1)\n",
    "        next_predicted = model.predict(current_sequence)\n",
    "        predicted_next_days.append((current_date, next_predicted[0, 0]))\n",
    "        next_predicted = next_predicted.reshape((1, 1, 1))\n",
    "        current_sequence = np.append(current_sequence[:, 1:, :], next_predicted, axis=1)\n",
    "\n",
    "\n",
    "# Escalar de volta as previsões para os valores originais\n",
    "predicted_values = np.array([value for _, value in predicted_next_days if value is not None])\n",
    "predicted_values_scaled_back = scaler.inverse_transform(predicted_values.reshape(-1, 1))\n",
    "\n",
    "# Associar as previsões com as datas\n",
    "predictions_with_dates = []\n",
    "for (current_date, _), value in zip(predicted_next_days, predicted_values_scaled_back):\n",
    "    predictions_with_dates.append((current_date.date(), value[0] if value is not None else None))\n",
    "\n",
    "# Printar as previsões com as datas correspondentes\n",
    "for date, value in predictions_with_dates:\n",
    "    if value is not None:\n",
    "        print(f'Data: {date}, Valor previsto: {value:.2f}')\n",
    "    else:\n",
    "        print(f'Data: {date}, Previsão não realizada para fins de semana.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "47/47 [==============================] - 4s 38ms/step - loss: 0.0521 - val_loss: 0.0362\n",
      "Epoch 2/100\n",
      "47/47 [==============================] - 1s 29ms/step - loss: 0.0439 - val_loss: 0.0367\n",
      "Epoch 3/100\n",
      "47/47 [==============================] - 1s 29ms/step - loss: 0.0412 - val_loss: 0.0320\n",
      "Epoch 4/100\n",
      "47/47 [==============================] - 1s 28ms/step - loss: 0.0369 - val_loss: 0.0273\n",
      "Epoch 5/100\n",
      "47/47 [==============================] - 1s 28ms/step - loss: 0.0334 - val_loss: 0.0291\n",
      "Epoch 6/100\n",
      "47/47 [==============================] - 1s 28ms/step - loss: 0.0305 - val_loss: 0.0243\n",
      "Epoch 7/100\n",
      "47/47 [==============================] - 1s 28ms/step - loss: 0.0288 - val_loss: 0.0248\n",
      "Epoch 8/100\n",
      "47/47 [==============================] - 1s 29ms/step - loss: 0.0260 - val_loss: 0.0220\n",
      "Epoch 9/100\n",
      "47/47 [==============================] - 1s 28ms/step - loss: 0.0249 - val_loss: 0.0236\n",
      "Epoch 10/100\n",
      "47/47 [==============================] - 1s 29ms/step - loss: 0.0240 - val_loss: 0.0207\n",
      "Epoch 11/100\n",
      "47/47 [==============================] - 1s 28ms/step - loss: 0.0219 - val_loss: 0.0178\n",
      "Epoch 12/100\n",
      "47/47 [==============================] - 1s 29ms/step - loss: 0.0203 - val_loss: 0.0169\n",
      "Epoch 13/100\n",
      "47/47 [==============================] - 1s 29ms/step - loss: 0.0201 - val_loss: 0.0195\n",
      "Epoch 14/100\n",
      "47/47 [==============================] - 1s 30ms/step - loss: 0.0187 - val_loss: 0.0158\n",
      "Epoch 15/100\n",
      "47/47 [==============================] - 1s 30ms/step - loss: 0.0172 - val_loss: 0.0172\n",
      "Epoch 16/100\n",
      "47/47 [==============================] - 2s 32ms/step - loss: 0.0167 - val_loss: 0.0133\n",
      "Epoch 17/100\n",
      "47/47 [==============================] - 2s 33ms/step - loss: 0.0158 - val_loss: 0.0127\n",
      "Epoch 18/100\n",
      "47/47 [==============================] - 1s 30ms/step - loss: 0.0157 - val_loss: 0.0121\n",
      "Epoch 19/100\n",
      "13/47 [=======>......................] - ETA: 0s - loss: 0.0146"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\milen\\OneDrive\\Documentos\\GitHub\\TCC\\lsmt.ipynb Cell 4\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/milen/OneDrive/Documentos/GitHub/TCC/lsmt.ipynb#W1sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m early_stopping \u001b[39m=\u001b[39m EarlyStopping(monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/milen/OneDrive/Documentos/GitHub/TCC/lsmt.ipynb#W1sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39m# Treinar o modelo\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/milen/OneDrive/Documentos/GitHub/TCC/lsmt.ipynb#W1sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m, callbacks\u001b[39m=\u001b[39;49m[early_stopping], validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/milen/OneDrive/Documentos/GitHub/TCC/lsmt.ipynb#W1sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39m# Carregar os dados de resposta que contêm apenas informações climáticas\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/milen/OneDrive/Documentos/GitHub/TCC/lsmt.ipynb#W1sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m df_response \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mC:/Users/milen/OneDrive/Documentos/TCC/Bases/respostas.csv\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\milen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\milen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1735\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1736\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1739\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1740\u001b[0m ):\n\u001b[0;32m   1741\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1742\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1743\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1744\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\milen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\milen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\milen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    854\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    855\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    856\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_no_variable_creation_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    859\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    860\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\milen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    146\u001b[0m   (concrete_function,\n\u001b[0;32m    147\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    149\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\milen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1345\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1346\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1347\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1348\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1349\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function(\u001b[39m*\u001b[39;49margs))\n\u001b[0;32m   1350\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m     args,\n\u001b[0;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1353\u001b[0m     executing_eagerly)\n\u001b[0;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\milen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[0;32m    195\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 196\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[0;32m    198\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[0;32m    199\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[0;32m    200\u001b[0m     )\n\u001b[0;32m    201\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\u001b[39mself\u001b[39m, \u001b[39mlist\u001b[39m(args))\n",
      "File \u001b[1;32mc:\\Users\\milen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[0;32m   1456\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m   1458\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1459\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[0;32m   1460\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[0;32m   1461\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m   1462\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1463\u001b[0m   )\n\u001b[0;32m   1464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1466\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   1467\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1471\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[0;32m   1472\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\milen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Carregar os dados de treino que incluem preços e dados climáticos\n",
    "df_train = pd.read_csv('C:/Users/milen/OneDrive/Documentos/TCC/Bases/base_preview.csv')\n",
    "df_train.fillna(method='ffill', inplace=True)\n",
    "df_train['Data'] = pd.to_datetime(df_train['Data'])\n",
    "\n",
    "# Selecionar as colunas de dados climáticos e de preço\n",
    "climate_features = ['PT1', 'PA1', 'TPO1', 'TMA1', 'TME1', 'TMI1', 'URA1', 'URAMI1', 'VRMA1', 'VVME1', 'PT2', 'PA2', 'TPO2', 'TMA2', 'TME2', 'TMI2', 'URA2', 'URAMI2', 'VRMA2', 'VVME2', 'PT4', 'PA4', 'TPO4', 'TMA4', 'TME4', 'TMI4', 'URA4', 'URAMI4', 'VRMA4', 'VVME4', 'PT6', 'PA6', 'TPO6', 'TMA6', 'TME6', 'TMI6', 'URA6', 'URAMI6', 'VRMA6', 'VVME6']  # Substitua pelos nomes reais das características climáticas\n",
    "target_column = 'Preco_Real'\n",
    "features = climate_features + [target_column]\n",
    "\n",
    "# Escalar os dados\n",
    "scaler_features = MinMaxScaler()\n",
    "scaler_target = MinMaxScaler()\n",
    "\n",
    "df_train_scaled = df_train.copy()\n",
    "df_train_scaled[features] = scaler_features.fit_transform(df_train[features])\n",
    "\n",
    "# Função para criar um conjunto de dados que inclui os dados climáticos e o preço\n",
    "def create_dataset(X, y, time_steps=1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        v = X[i:(i + time_steps), :]\n",
    "        Xs.append(v)\n",
    "        ys.append(y[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "time_steps = 30  # Use 30 dias para criar a sequência\n",
    "\n",
    "# Preparar o conjunto de dados de entrada e saída\n",
    "X, y = create_dataset(df_train_scaled[climate_features].values, df_train_scaled[target_column].values, time_steps)\n",
    "\n",
    "# Dividir os dados em conjuntos de treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Modelo\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=70, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(LSTM(units=70, return_sequences=False))\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "# Compilar o modelo\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='mean_squared_error')\n",
    "\n",
    "# Adicionar Early Stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "# Treinar o modelo\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=64, callbacks=[early_stopping], validation_split=0.1)\n",
    "\n",
    "# Carregar os dados de resposta que contêm apenas informações climáticas\n",
    "df_response = pd.read_csv('C:/Users/milen/OneDrive/Documentos/TCC/Bases/respostas.csv')\n",
    "df_response['Data'] = pd.to_datetime(df_response['Data'])\n",
    "df_response_scaled = scaler_features.transform(df_response[climate_features])\n",
    "\n",
    "# Criar o conjunto de dados de entrada para previsão\n",
    "X_response = []\n",
    "for i in range(len(df_response_scaled) - time_steps):\n",
    "    X_response.append(df_response_scaled[i:(i + time_steps), :-1])\n",
    "X_response = np.array(X_response)\n",
    "\n",
    "# Prever os próximos valores\n",
    "predicted_prices_scaled = model.predict(X_response)\n",
    "predicted_prices = scaler_features.inverse_transform(np.hstack((predicted_prices_scaled, np.zeros((predicted_prices_scaled.shape[0], len(climate_features) - 1)))))\n",
    "\n",
    "# Exibir as previsões\n",
    "predicted_prices = predicted_prices[:, 0]  # Selecionar apenas a coluna de preços\n",
    "for i, price in enumerate(predicted_prices):\n",
    "    print(f'Previsão {i+1}: {price:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The DType <class 'numpy.dtype[datetime64]'> could not be promoted by <class 'numpy.dtype[float64]'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtype[datetime64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\milen\\OneDrive\\Documentos\\GitHub\\TCC\\lsmt.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/milen/OneDrive/Documentos/GitHub/TCC/lsmt.ipynb#W4sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m scaler_features \u001b[39m=\u001b[39m MinMaxScaler()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/milen/OneDrive/Documentos/GitHub/TCC/lsmt.ipynb#W4sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# Fit the scaler on your initial dataset without 'Preco_Real'\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/milen/OneDrive/Documentos/GitHub/TCC/lsmt.ipynb#W4sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m scaler_features\u001b[39m.\u001b[39;49mfit(df_initial[climate_features])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/milen/OneDrive/Documentos/GitHub/TCC/lsmt.ipynb#W4sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# Load new data to be used for predictions\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/milen/OneDrive/Documentos/GitHub/TCC/lsmt.ipynb#W4sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m df_response \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mC:/Users/milen/OneDrive/Documentos/TCC/Bases/respostas.csv\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\milen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:427\u001b[0m, in \u001b[0;36mMinMaxScaler.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[39m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[1;32m--> 427\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpartial_fit(X, y)\n",
      "File \u001b[1;32mc:\\Users\\milen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:466\u001b[0m, in \u001b[0;36mMinMaxScaler.partial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    460\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    461\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMinMaxScaler does not support sparse input. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    462\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConsider using MaxAbsScaler instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    463\u001b[0m     )\n\u001b[0;32m    465\u001b[0m first_pass \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mn_samples_seen_\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 466\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    467\u001b[0m     X,\n\u001b[0;32m    468\u001b[0m     reset\u001b[39m=\u001b[39;49mfirst_pass,\n\u001b[0;32m    469\u001b[0m     dtype\u001b[39m=\u001b[39;49mFLOAT_DTYPES,\n\u001b[0;32m    470\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    471\u001b[0m )\n\u001b[0;32m    473\u001b[0m data_min \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mnanmin(X, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m    474\u001b[0m data_max \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mnanmax(X, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\milen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:565\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    563\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    564\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 565\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    566\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[0;32m    567\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32mc:\\Users\\milen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:778\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    774\u001b[0m     pandas_requires_conversion \u001b[39m=\u001b[39m \u001b[39many\u001b[39m(\n\u001b[0;32m    775\u001b[0m         _pandas_dtype_needs_early_conversion(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m dtypes_orig\n\u001b[0;32m    776\u001b[0m     )\n\u001b[0;32m    777\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(dtype_iter, np\u001b[39m.\u001b[39mdtype) \u001b[39mfor\u001b[39;00m dtype_iter \u001b[39min\u001b[39;00m dtypes_orig):\n\u001b[1;32m--> 778\u001b[0m         dtype_orig \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mresult_type(\u001b[39m*\u001b[39;49mdtypes_orig)\n\u001b[0;32m    780\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mhasattr\u001b[39m(array, \u001b[39m\"\u001b[39m\u001b[39miloc\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(array, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    781\u001b[0m     \u001b[39m# array is a pandas series\u001b[39;00m\n\u001b[0;32m    782\u001b[0m     pandas_requires_conversion \u001b[39m=\u001b[39m _pandas_dtype_needs_early_conversion(array\u001b[39m.\u001b[39mdtype)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mresult_type\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: The DType <class 'numpy.dtype[datetime64]'> could not be promoted by <class 'numpy.dtype[float64]'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtype[datetime64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>, <class 'numpy.dtype[float64]'>)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib  # Updated import statement\n",
    "\n",
    "# Assuming that 'climate_features' and 'your_model.pkl' paths are correct\n",
    "climate_features = ['Data', 'PT1', 'PA1', 'TPO1', 'TMA1', 'TME1', 'TMI1', 'URA1', 'URAMI1', 'VRMA1', 'VVME1', 'PT2', 'PA2', 'TPO2', 'TMA2', 'TME2', 'TMI2', 'URA2', 'URAMI2', 'VRMA2', 'VVME2', 'PT4', 'PA4', 'TPO4', 'TMA4', 'TME4', 'TMI4', 'URA4', 'URAMI4', 'VRMA4', 'VVME4', 'PT6', 'PA6', 'TPO6', 'TMA6', 'TME6', 'TMI6', 'URA6', 'URAMI6', 'VRMA6', 'VVME6']\n",
    "\n",
    "# Load initial dataset to fit the scaler\n",
    "df_initial = pd.read_csv('C:/Users/milen/OneDrive/Documentos/TCC/Bases/base_preview.csv')\n",
    "df_initial['Data'] = pd.to_datetime(df_initial['Data'])\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler_features = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler on your initial dataset without 'Preco_Real'\n",
    "scaler_features.fit(df_initial[climate_features])\n",
    "\n",
    "# Load new data to be used for predictions\n",
    "df_response = pd.read_csv('C:/Users/milen/OneDrive/Documentos/TCC/Bases/respostas.csv')\n",
    "df_response['Data'] = pd.to_datetime(df_response['Data'])\n",
    "\n",
    "# Scale the new data with the fitted scaler\n",
    "df_response_scaled = scaler_features.transform(df_response[climate_features])\n",
    "\n",
    "# Load your trained model\n",
    "model = joblib.load('your_model.pkl')  # Replace with your model's filename\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(df_response_scaled)\n",
    "\n",
    "# Add predictions to the DataFrame\n",
    "df_response['Preco_Real_Predicted'] = predictions\n",
    "\n",
    "# Output the DataFrame with the predictions\n",
    "df_response.to_csv('predictions.csv', index=False)  # Save the predictions to a CSV file\n",
    "\n",
    "print(\"Predictions have been saved to 'predictions.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            TMI_media  Preco_Real\n",
      "TMI_media    1.000000    0.012706\n",
      "Preco_Real   0.012706    1.000000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not interpret value `preco_real` for `y`. An entry with this name does not appear in `data`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\milen\\OneDrive\\Documentos\\GitHub\\TCC\\lsmt.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/milen/OneDrive/Documentos/GitHub/TCC/lsmt.ipynb#W5sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# Para visualização, vamos plotar um gráfico de dispersão\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/milen/OneDrive/Documentos/GitHub/TCC/lsmt.ipynb#W5sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m10\u001b[39m, \u001b[39m6\u001b[39m))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/milen/OneDrive/Documentos/GitHub/TCC/lsmt.ipynb#W5sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m sns\u001b[39m.\u001b[39;49mscatterplot(data\u001b[39m=\u001b[39;49mdf, x\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mTMI_media\u001b[39;49m\u001b[39m'\u001b[39;49m, y\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mpreco_real\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/milen/OneDrive/Documentos/GitHub/TCC/lsmt.ipynb#W5sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m plt\u001b[39m.\u001b[39mtitle(\u001b[39m'\u001b[39m\u001b[39mCorrelação entre Temperatura Média Mínima e Preço Real do Café\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/milen/OneDrive/Documentos/GitHub/TCC/lsmt.ipynb#W5sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m plt\u001b[39m.\u001b[39mxlabel(\u001b[39m'\u001b[39m\u001b[39mTemperatura Média Mínima (°C)\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\milen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\seaborn\\relational.py:603\u001b[0m, in \u001b[0;36mscatterplot\u001b[1;34m(data, x, y, hue, size, style, palette, hue_order, hue_norm, sizes, size_order, size_norm, markers, style_order, legend, ax, **kwargs)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mscatterplot\u001b[39m(\n\u001b[0;32m    595\u001b[0m     data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m,\n\u001b[0;32m    596\u001b[0m     x\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, hue\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, size\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, style\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    600\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    601\u001b[0m ):\n\u001b[1;32m--> 603\u001b[0m     p \u001b[39m=\u001b[39m _ScatterPlotter(\n\u001b[0;32m    604\u001b[0m         data\u001b[39m=\u001b[39;49mdata,\n\u001b[0;32m    605\u001b[0m         variables\u001b[39m=\u001b[39;49m\u001b[39mdict\u001b[39;49m(x\u001b[39m=\u001b[39;49mx, y\u001b[39m=\u001b[39;49my, hue\u001b[39m=\u001b[39;49mhue, size\u001b[39m=\u001b[39;49msize, style\u001b[39m=\u001b[39;49mstyle),\n\u001b[0;32m    606\u001b[0m         legend\u001b[39m=\u001b[39;49mlegend\n\u001b[0;32m    607\u001b[0m     )\n\u001b[0;32m    609\u001b[0m     p\u001b[39m.\u001b[39mmap_hue(palette\u001b[39m=\u001b[39mpalette, order\u001b[39m=\u001b[39mhue_order, norm\u001b[39m=\u001b[39mhue_norm)\n\u001b[0;32m    610\u001b[0m     p\u001b[39m.\u001b[39mmap_size(sizes\u001b[39m=\u001b[39msizes, order\u001b[39m=\u001b[39msize_order, norm\u001b[39m=\u001b[39msize_norm)\n",
      "File \u001b[1;32mc:\\Users\\milen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\seaborn\\relational.py:390\u001b[0m, in \u001b[0;36m_ScatterPlotter.__init__\u001b[1;34m(self, data, variables, legend)\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, variables\u001b[39m=\u001b[39m{}, legend\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    382\u001b[0m \n\u001b[0;32m    383\u001b[0m     \u001b[39m# TODO this is messy, we want the mapping to be agnostic about\u001b[39;00m\n\u001b[0;32m    384\u001b[0m     \u001b[39m# the kind of plot to draw, but for the time being we need to set\u001b[39;00m\n\u001b[0;32m    385\u001b[0m     \u001b[39m# this information so the SizeMapping can use it\u001b[39;00m\n\u001b[0;32m    386\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_default_size_range \u001b[39m=\u001b[39m (\n\u001b[0;32m    387\u001b[0m         np\u001b[39m.\u001b[39mr_[\u001b[39m.5\u001b[39m, \u001b[39m2\u001b[39m] \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39msquare(mpl\u001b[39m.\u001b[39mrcParams[\u001b[39m\"\u001b[39m\u001b[39mlines.markersize\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m    388\u001b[0m     )\n\u001b[1;32m--> 390\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(data\u001b[39m=\u001b[39;49mdata, variables\u001b[39m=\u001b[39;49mvariables)\n\u001b[0;32m    392\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlegend \u001b[39m=\u001b[39m legend\n",
      "File \u001b[1;32mc:\\Users\\milen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\seaborn\\_base.py:634\u001b[0m, in \u001b[0;36mVectorPlotter.__init__\u001b[1;34m(self, data, variables)\u001b[0m\n\u001b[0;32m    629\u001b[0m \u001b[39m# var_ordered is relevant only for categorical axis variables, and may\u001b[39;00m\n\u001b[0;32m    630\u001b[0m \u001b[39m# be better handled by an internal axis information object that tracks\u001b[39;00m\n\u001b[0;32m    631\u001b[0m \u001b[39m# such information and is set up by the scale_* methods. The analogous\u001b[39;00m\n\u001b[0;32m    632\u001b[0m \u001b[39m# information for numeric axes would be information about log scales.\u001b[39;00m\n\u001b[0;32m    633\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_var_ordered \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mx\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mFalse\u001b[39;00m}  \u001b[39m# alt., used DefaultDict\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49massign_variables(data, variables)\n\u001b[0;32m    636\u001b[0m \u001b[39m# TODO Lots of tests assume that these are called to initialize the\u001b[39;00m\n\u001b[0;32m    637\u001b[0m \u001b[39m# mappings to default values on class initialization. I'd prefer to\u001b[39;00m\n\u001b[0;32m    638\u001b[0m \u001b[39m# move away from that and only have a mapping when explicitly called.\u001b[39;00m\n\u001b[0;32m    639\u001b[0m \u001b[39mfor\u001b[39;00m var \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mhue\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msize\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstyle\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\milen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\seaborn\\_base.py:679\u001b[0m, in \u001b[0;36mVectorPlotter.assign_variables\u001b[1;34m(self, data, variables)\u001b[0m\n\u001b[0;32m    674\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    675\u001b[0m     \u001b[39m# When dealing with long-form input, use the newer PlotData\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[39m# object (internal but introduced for the objects interface)\u001b[39;00m\n\u001b[0;32m    677\u001b[0m     \u001b[39m# to centralize / standardize data consumption logic.\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_format \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlong\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 679\u001b[0m     plot_data \u001b[39m=\u001b[39m PlotData(data, variables)\n\u001b[0;32m    680\u001b[0m     frame \u001b[39m=\u001b[39m plot_data\u001b[39m.\u001b[39mframe\n\u001b[0;32m    681\u001b[0m     names \u001b[39m=\u001b[39m plot_data\u001b[39m.\u001b[39mnames\n",
      "File \u001b[1;32mc:\\Users\\milen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\seaborn\\_core\\data.py:58\u001b[0m, in \u001b[0;36mPlotData.__init__\u001b[1;34m(self, data, variables)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m     52\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     53\u001b[0m     data: DataSource,\n\u001b[0;32m     54\u001b[0m     variables: \u001b[39mdict\u001b[39m[\u001b[39mstr\u001b[39m, VariableSpec],\n\u001b[0;32m     55\u001b[0m ):\n\u001b[0;32m     57\u001b[0m     data \u001b[39m=\u001b[39m handle_data_source(data)\n\u001b[1;32m---> 58\u001b[0m     frame, names, ids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_assign_variables(data, variables)\n\u001b[0;32m     60\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframe \u001b[39m=\u001b[39m frame\n\u001b[0;32m     61\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnames \u001b[39m=\u001b[39m names\n",
      "File \u001b[1;32mc:\\Users\\milen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\seaborn\\_core\\data.py:232\u001b[0m, in \u001b[0;36mPlotData._assign_variables\u001b[1;34m(self, data, variables)\u001b[0m\n\u001b[0;32m    230\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    231\u001b[0m         err \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mAn entry with this name does not appear in `data`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 232\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(err)\n\u001b[0;32m    234\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    235\u001b[0m \n\u001b[0;32m    236\u001b[0m     \u001b[39m# Otherwise, assume the value somehow represents data\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \n\u001b[0;32m    238\u001b[0m     \u001b[39m# Ignore empty data structures\u001b[39;00m\n\u001b[0;32m    239\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(val, Sized) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(val) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: Could not interpret value `preco_real` for `y`. An entry with this name does not appear in `data`."
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Substitua 'caminho_para_seu_arquivo.csv' pelo caminho do seu arquivo CSV\n",
    "precos_df = pd.read_csv('C:/Users/milen/OneDrive/Documentos/TCC/Bases/base_preview.csv')\n",
    "\n",
    "# Combina o DataFrame de preços com o DataFrame de temperaturas baseando-se na coluna 'Data'\n",
    "# Isso pressupõe que ambos os DataFrames têm uma coluna 'Data' que pode ser usada para unir os dados\n",
    "df['Data'] = pd.to_datetime(df['Data'])\n",
    "precos_df['Data'] = pd.to_datetime(precos_df['Data'])\n",
    "df = pd.merge(df, precos_df, on='Data', how='inner')\n",
    "\n",
    "# Calcula a média diária das temperaturas mínimas\n",
    "df['TMI_media'] = df[['TMI1', 'TMI2', 'TMI4', 'TMI6']].mean(axis=1)\n",
    "\n",
    "# Vamos supor que você tenha uma coluna 'preco_real' em seu dataframe\n",
    "# df['preco_real'] = [...seus dados de preço real...]\n",
    "\n",
    "# Agora, calculamos a correlação\n",
    "correlacao = df[['TMI_media', 'Preco_Real']].corr()\n",
    "\n",
    "# Imprime a matriz de correlação\n",
    "print(correlacao)\n",
    "\n",
    "# Para visualização, vamos plotar um gráfico de dispersão\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=df, x='TMI_media', y='preco_real')\n",
    "plt.title('Correlação entre Temperatura Média Mínima e Preço Real do Café')\n",
    "plt.xlabel('Temperatura Média Mínima (°C)')\n",
    "plt.ylabel('Preço Real do Café')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
